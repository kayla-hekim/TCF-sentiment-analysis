{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c980497b",
   "metadata": {},
   "source": [
    "# Generalizing Sentiment Analysis based on the given course URL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4171696",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e013080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#EDA stuff:\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "#natural language tokenizer:\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "#Beautiful soup use\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://thecourseforum.com/course/14942/7737/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "# Roberta Model\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa7b84",
   "metadata": {},
   "source": [
    "## web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0643af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_page_num(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve webpage\")\n",
    "        return 1\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pagination_div = soup.find('div', class_='pagination')\n",
    "    if not pagination_div:\n",
    "        print('error on first')\n",
    "        return 1\n",
    "\n",
    "    page_links = pagination_div.find_all('a', href=lambda href: href and 'page=' in href)\n",
    "    if not page_links:\n",
    "        print('error on second')\n",
    "        return 1\n",
    "\n",
    "    last_link = page_links[-1]\n",
    "    last_page_href = last_link.get('href')\n",
    "    \n",
    "    try:\n",
    "        page_number = last_page_href.split('?page=')[1].split('#reviews')[0]\n",
    "        return int(page_number)\n",
    "    except (IndexError, ValueError):\n",
    "        print('error on last')\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46691ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_reviews(url):\n",
    "    all_reviews_final = []\n",
    "    all_ratings_final = []\n",
    "\n",
    "    last_page_num = get_last_page_num(url)\n",
    "    # print(last_page_num)\n",
    "\n",
    "    for i in range(1, (int(last_page_num) + 1)):\n",
    "        new_url = url + f\"?page={i}#reviews\"\n",
    "        # print(\"scraping page\", new_url)\n",
    "\n",
    "        response = requests.get(new_url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find the main container for all reviews (the <ul> list) based on the class 'review-list list-unstyled'\n",
    "            review_list = soup.find('ul', class_='review-list list-unstyled')\n",
    "\n",
    "            # Check if the review list was found\n",
    "            if review_list:\n",
    "                all_reviews = review_list.find_all('li')\n",
    "\n",
    "                # print(f\"Found {len(all_reviews)} reviews for page {i}.\")\n",
    "\n",
    "                for j, review_item in enumerate(all_reviews):\n",
    "                    review_text_div = review_item.find('div', class_='review-text-full')\n",
    "                    \n",
    "                    if review_text_div:\n",
    "                        review_text = review_text_div.get_text(strip=True)\n",
    "                        # print(f\"\\n--- Review {j+1} ---\\n{review_text}\")\n",
    "                        all_reviews_final.append(review_text)\n",
    "            else:\n",
    "                print(\"Could not find the review list container.\")\n",
    "                break\n",
    "\n",
    "\n",
    "            # Check if the review list was found\n",
    "            if review_list:\n",
    "                all_ratings = review_list.find_all('li')\n",
    "\n",
    "                for review_item in all_ratings:\n",
    "                    rating_div = review_item.find('div', id='review-average')\n",
    "                    \n",
    "                    if rating_div:\n",
    "                        rating = rating_div.get_text(strip=True)\n",
    "                        # print(rating)\n",
    "                        all_ratings_final.append(float(rating))\n",
    "            else:\n",
    "                print(\"Could not find the rating container.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "    return all_reviews_final, all_ratings_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ce8f4",
   "metadata": {},
   "source": [
    "## roberta model and utilization (generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a4d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example, tokenizer, model):\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg' : scores[0],\n",
    "        'roberta_neu' : scores[1],\n",
    "        'roberta_pos' : scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cfb26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_roberta (df, sia):\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    res = {}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            text = row['review_text']\n",
    "            # rating = row['rating']\n",
    "            myid = row['Id']\n",
    "            vader_result = sia.polarity_scores(text)\n",
    "\n",
    "            vader_result_rename = {}\n",
    "            for key, value in vader_result.items(): # rename vader\n",
    "                vader_result_rename[f\"vader_{key}\"] = value\n",
    "\n",
    "            roberta_result = polarity_scores_roberta(text, tokenizer, model)\n",
    "            both = {**vader_result_rename, **roberta_result}\n",
    "            res[myid] = both\n",
    "\n",
    "            results_df = pd.DataFrame(res).T\n",
    "            results_df = results_df.reset_index().rename(columns={'index' : 'Id'})\n",
    "            results_df = results_df.merge(df, how='left')\n",
    "\n",
    "        except RuntimeError: # stating that the roberta model breaks for examples where text is too long -> print in some way\n",
    "            print(f'Broke for id {myid}')\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84a4ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeb5eca78b4421aad994f0925bfb240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6078586206896552\n"
     ]
    }
   ],
   "source": [
    "# input: url to course, modelType (huggingface auto, built one, etc.)\n",
    "# output: average compound score for course, list of review_texts, list of ratings\n",
    "def general_TCF_Sentiment_analyzer(url):\n",
    "    all_reviews_list = []\n",
    "    all_ratings_list = []\n",
    "    all_reviews_list, all_ratings_list = scrape_all_reviews(url)\n",
    "    # print(len(all_ratings_list), len(all_reviews_list))\n",
    "\n",
    "    future_id = list(range(len(all_reviews_list)))\n",
    "    data = {'Id': future_id, 'review_text': all_reviews_list, 'rating': all_ratings_list}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    results_df_passed = train_roberta(df, sia)\n",
    "\n",
    "    list_compounds = results_df_passed['vader_compound']\n",
    "    average_compound_score = list_compounds.mean()\n",
    "\n",
    "\n",
    "    # return average_compound_score, all_reviews_list, all_ratings_list\n",
    "    return average_compound_score, all_reviews_list, all_ratings_list # TEMP\n",
    "\n",
    "average_compound_score, all_reviews_list, all_ratings_list = general_TCF_Sentiment_analyzer(url)\n",
    "print(average_compound_score) # 0.6078586206896552 should be for cso1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0f082",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b933a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2994db337104653acc4bf9aacfd8d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broke for id 9\n",
      "0.34205833333333335\n"
     ]
    }
   ],
   "source": [
    "url = \"https://thecourseforum.com/course/15227/4710/\"\n",
    "average_compound_score_dmt2, new_reviews_dmt2, new_ratings_dmt2 = general_TCF_Sentiment_analyzer(url)\n",
    "print(average_compound_score_dmt2)\n",
    "\n",
    "# for each in new_reviews_dmt2:\n",
    "#     print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a405c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on second\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f56acc846c42df9eb05dddcd719ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7747\n"
     ]
    }
   ],
   "source": [
    "url = \"https://thecourseforum.com/course/15790/13128/\"\n",
    "average_compound_score_compt_prob, new_reviews_comp_prob, new_ratings_comp_prob = general_TCF_Sentiment_analyzer(url)\n",
    "print(average_compound_score_compt_prob)\n",
    "\n",
    "# for each in new_ratings_comp_prob:\n",
    "#     print(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48838e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2319cbc611d3415e988b67c7036271db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5659292978208232\n"
     ]
    }
   ],
   "source": [
    "url = \"https://thecourseforum.com/course/748/691/\"\n",
    "average_compound_score_econ, new_reviews_econ, new_ratings_econ = general_TCF_Sentiment_analyzer(url)\n",
    "print(average_compound_score_econ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "404a8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://thecourseforum.com/course/15909/11449/\"\n",
    "# average_compound_score_ml1, new_reviews_ml1, new_ratings_ml1 = general_TCF_Sentiment_analyzer(url)\n",
    "# print(average_compound_score_ml1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd44700",
   "metadata": {},
   "source": [
    "## ISSUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bd81a",
   "metadata": {},
   "source": [
    "**1.**\n",
    "\n",
    "[RESOLVED: error in not providing local soup to get_last_page_num()] So, the compound scores were found to be accurate through the courseForumWebscrape.ipynb file. However it's duplicating some of the reviews and ratings in the list. The important thing is that the compound scores (average and individuals) are accurate, and that there are equal number of ratings and reviews per course. \n",
    "\n",
    "I suspect this issue of duplicating stuff, as seen if you uncomment the last code cell above from comp prob, is that sometimes people rate courses, but don't leave a review. So the loop in scrape_all_reviews will not detect a review, and use the last found review on the rating found. This is due to, then, how we web scrape and not the sentiment analysis model itself.\n",
    "\n",
    "That scrape_all_reviews method could be simplified and \"debugged\" in the future. However, for the purposes of this project, we've compounded the sentiment scores and can list the individual review sentiment analysis scores if we needed.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**2.**\n",
    "\n",
    "If you uncomment the last cell about ml1, there'll be an error. Currently, this course doesn't have any reviews or ratings. I could easily go in and fix this, but I haven't gotten the chance.\n",
    "\n",
    "I suspect I'd provide get_last_page_num a check for if there are any reviews or ratings. If either don't exist (len(ratings)==0 | len(reviews)==0), just return or print for debugging. This might break something that was intentional in this project, so I'm leaving it for now and to fix later when the chance arises.\n",
    "<br><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
