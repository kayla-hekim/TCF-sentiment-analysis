{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f339086a",
   "metadata": {},
   "source": [
    "# Encapsulating Summary with NLTK\n",
    "### We'll be learning how to use forum reviews to form one encapsulating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f302f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kaylakim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "url = 'https://thecourseforum.com/course/14942/7737/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690d375",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8d7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_page_num(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve webpage\")\n",
    "        return 1\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pagination_div = soup.find('div', class_='pagination')\n",
    "    if not pagination_div:\n",
    "        print('error on first')\n",
    "        return 1\n",
    "\n",
    "    page_links = pagination_div.find_all('a', href=lambda href: href and 'page=' in href)\n",
    "    if not page_links:\n",
    "        print('error on second')\n",
    "        return 1\n",
    "\n",
    "    last_link = page_links[-1]\n",
    "    last_page_href = last_link.get('href')\n",
    "    \n",
    "    try:\n",
    "        page_number = last_page_href.split('?page=')[1].split('#reviews')[0]\n",
    "        return int(page_number)\n",
    "    except (IndexError, ValueError):\n",
    "        print('error on last')\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad7fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_reviews(url):\n",
    "    all_reviews_final = []\n",
    "    all_ratings_final = []\n",
    "\n",
    "    last_page_num = get_last_page_num(url)\n",
    "    # print(last_page_num)\n",
    "\n",
    "    for i in range(1, (int(last_page_num) + 1)):\n",
    "        new_url = url + f\"?page={i}#reviews\"\n",
    "        # print(\"scraping page\", new_url)\n",
    "\n",
    "        response = requests.get(new_url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find the main container for all reviews (the <ul> list) based on the class 'review-list list-unstyled'\n",
    "            review_list = soup.find('ul', class_='review-list list-unstyled')\n",
    "\n",
    "            # Check if the review list was found\n",
    "            if review_list:\n",
    "                all_reviews = review_list.find_all('li')\n",
    "\n",
    "                # print(f\"Found {len(all_reviews)} reviews for page {i}.\")\n",
    "\n",
    "                for j, review_item in enumerate(all_reviews):\n",
    "                    review_text_div = review_item.find('div', class_='review-text-full')\n",
    "                    \n",
    "                    if review_text_div:\n",
    "                        review_text = review_text_div.get_text(strip=True)\n",
    "                        # print(f\"\\n--- Review {j+1} ---\\n{review_text}\")\n",
    "                        all_reviews_final.append(review_text)\n",
    "            else:\n",
    "                print(\"Could not find the review list container.\")\n",
    "                break\n",
    "\n",
    "\n",
    "            # Check if the review list was found\n",
    "            if review_list:\n",
    "                all_ratings = review_list.find_all('li')\n",
    "\n",
    "                for review_item in all_ratings:\n",
    "                    rating_div = review_item.find('div', id='review-average')\n",
    "                    \n",
    "                    if rating_div:\n",
    "                        rating = rating_div.get_text(strip=True)\n",
    "                        # print(rating)\n",
    "                        all_ratings_final.append(float(rating))\n",
    "            else:\n",
    "                print(\"Could not find the rating container.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "    return all_reviews_final, all_ratings_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51421800",
   "metadata": {},
   "source": [
    "## Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7754e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example, tokenizer, model):\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'roberta_neg' : scores[0],\n",
    "        'roberta_neu' : scores[1],\n",
    "        'roberta_pos' : scores[2]\n",
    "    }\n",
    "    return scores_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de69439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_roberta (df, sia):\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    res = {}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            text = row['review_text']\n",
    "            # rating = row['rating']\n",
    "            myid = row['Id']\n",
    "            vader_result = sia.polarity_scores(text)\n",
    "\n",
    "            vader_result_rename = {}\n",
    "            for key, value in vader_result.items(): # rename vader\n",
    "                vader_result_rename[f\"vader_{key}\"] = value\n",
    "\n",
    "            roberta_result = polarity_scores_roberta(text, tokenizer, model)\n",
    "            both = {**vader_result_rename, **roberta_result}\n",
    "            res[myid] = both\n",
    "\n",
    "            results_df = pd.DataFrame(res).T\n",
    "            results_df = results_df.reset_index().rename(columns={'index' : 'Id'})\n",
    "            results_df = results_df.merge(df, how='left')\n",
    "\n",
    "        except RuntimeError: # stating that the roberta model breaks for examples where text is too long -> print in some way\n",
    "            print(f'Broke for id {myid}')\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ca9a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a452acecf1474e6199fdac30e0ad4d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6078586206896552\n"
     ]
    }
   ],
   "source": [
    "# input: url to course, modelType (huggingface auto, built one, etc.)\n",
    "# output: average compound score for course, list of review_texts, list of ratings\n",
    "def general_TCF_Sentiment_analyzer(url):\n",
    "    all_reviews_list = []\n",
    "    all_ratings_list = []\n",
    "    all_reviews_list, all_ratings_list = scrape_all_reviews(url)\n",
    "    # print(len(all_ratings_list), len(all_reviews_list))\n",
    "\n",
    "    future_id = list(range(len(all_reviews_list)))\n",
    "    data = {'Id': future_id, 'review_text': all_reviews_list, 'rating': all_ratings_list}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    results_df_passed = train_roberta(df, sia)\n",
    "\n",
    "    list_compounds = results_df_passed['vader_compound']\n",
    "    average_compound_score = list_compounds.mean()\n",
    "\n",
    "\n",
    "    # return average_compound_score, all_reviews_list, all_ratings_list\n",
    "    return average_compound_score, all_reviews_list, all_ratings_list # TEMP\n",
    "\n",
    "url = 'https://thecourseforum.com/course/14942/7737/'\n",
    "average_compound_score, all_reviews_list, all_ratings_list = general_TCF_Sentiment_analyzer(url)\n",
    "print(average_compound_score) # 0.6078586206896552 should be for cso1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a515e",
   "metadata": {},
   "source": [
    "## Generated Summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000b012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
